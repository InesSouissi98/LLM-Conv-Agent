{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh8I5T5o6aK6",
        "outputId": "2d012c58-e52b-4f41-de67-1751019c148b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 9.6 MB/s eta 0:00:00\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.3.2\n",
            "Collecting farm-haystack[colab]\n",
            "  Downloading farm_haystack-1.23.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting boilerpy3 (from farm-haystack[colab])\n",
            "  Downloading boilerpy3-1.0.7-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting events (from farm-haystack[colab])\n",
            "  Downloading Events-0.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting httpx (from farm-haystack[colab])\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab]) (4.19.2)\n",
            "Collecting lazy-imports==0.3.1 (from farm-haystack[colab])\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab]) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab]) (3.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab]) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab]) (9.4.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab]) (4.1.0)\n",
            "Collecting posthog (from farm-haystack[colab])\n",
            "  Downloading posthog-3.1.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting prompthub-py==4.0.0 (from farm-haystack[colab])\n",
            "  Downloading prompthub_py-4.0.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: pydantic<2 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab]) (1.10.13)\n",
            "Collecting quantulum3 (from farm-haystack[colab])\n",
            "  Downloading quantulum3-0.9.0-py3-none-any.whl (10.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.7/10.7 MB 88.2 MB/s eta 0:00:00\n",
            "Collecting rank-bm25 (from farm-haystack[colab])\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab]) (2.31.0)\n",
            "Collecting requests-cache<1.0.0 (from farm-haystack[colab])\n",
            "  Downloading requests_cache-0.9.8-py3-none-any.whl (48 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.7/48.7 kB 2.8 MB/s eta 0:00:00\n",
            "Collecting scikit-learn>=1.3.0 (from farm-haystack[colab])\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting sseclient-py (from farm-haystack[colab])\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab]) (8.2.3)\n",
            "Collecting tiktoken>=0.5.1 (from farm-haystack[colab])\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab]) (4.66.1)\n",
            "Requirement already satisfied: transformers==4.35.2 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab]) (4.35.2)\n",
            "Collecting pillow (from farm-haystack[colab])\n",
            "  Downloading Pillow-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/4.3 MB 75.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from prompthub-py==4.0.0->farm-haystack[colab]) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2->farm-haystack[colab]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2->farm-haystack[colab]) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2->farm-haystack[colab]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2->farm-haystack[colab]) (23.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2->farm-haystack[colab]) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2->farm-haystack[colab]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2->farm-haystack[colab]) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2->farm-haystack[colab]) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab]) (2023.11.17)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack[colab]) (1.4.4)\n",
            "Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack[colab]) (23.1.0)\n",
            "Collecting cattrs>=22.2 (from requests-cache<1.0.0->farm-haystack[colab])\n",
            "  Downloading cattrs-23.2.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting url-normalize>=1.4 (from requests-cache<1.0.0->farm-haystack[colab])\n",
            "  Downloading url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack[colab]) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack[colab]) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack[colab]) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->farm-haystack[colab]) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx->farm-haystack[colab])\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->farm-haystack[colab]) (1.3.0)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->farm-haystack[colab])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 4.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[colab]) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[colab]) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[colab]) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->farm-haystack[colab]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->farm-haystack[colab]) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->farm-haystack[colab]) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog->farm-haystack[colab])\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->farm-haystack[colab])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from quantulum3->farm-haystack[colab]) (7.0.0)\n",
            "Collecting num2words (from quantulum3->farm-haystack[colab])\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from cattrs>=22.2->requests-cache<1.0.0->farm-haystack[colab]) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->farm-haystack[colab]) (2023.6.0)\n",
            "Collecting docopt>=0.6.2 (from num2words->quantulum3->farm-haystack[colab])\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Downloading prompthub_py-4.0.0-py3-none-any.whl (6.9 kB)\n",
            "Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.8/10.8 MB 84.6 MB/s eta 0:00:00\n",
            "Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 66.0 MB/s eta 0:00:00\n",
            "Downloading boilerpy3-1.0.7-py3-none-any.whl (22 kB)\n",
            "Downloading Events-0.5-py3-none-any.whl (6.8 kB)\n",
            "Downloading farm_haystack-1.23.0-py3-none-any.whl (764 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 764.4/764.4 kB 40.4 MB/s eta 0:00:00\n",
            "Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.9/75.9 kB 5.1 MB/s eta 0:00:00\n",
            "Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.9/76.9 kB 5.5 MB/s eta 0:00:00\n",
            "Downloading posthog-3.1.0-py2.py3-none-any.whl (37 kB)\n",
            "Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading cattrs-23.2.3-py3-none-any.whl (57 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 3.9 MB/s eta 0:00:00\n",
            "Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.3/143.3 kB 11.2 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py): started\n",
            "  Building wheel for docopt (setup.py): finished with status 'done'\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=45b06ee1d624879bc68d28badf736c18c06b144dbda72051aeb688ff6c12275b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: sseclient-py, monotonic, events, docopt, url-normalize, rank-bm25, pillow, num2words, lazy-imports, h11, cattrs, boilerpy3, backoff, tiktoken, scikit-learn, requests-cache, prompthub-py, posthog, httpcore, quantulum3, httpx, farm-haystack\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed backoff-2.2.1 boilerpy3-1.0.7 cattrs-23.2.3 docopt-0.6.2 events-0.5 farm-haystack-1.23.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 lazy-imports-0.3.1 monotonic-1.6 num2words-0.5.13 pillow-9.0.0 posthog-3.1.0 prompthub-py-4.0.0 quantulum3-0.9.0 rank-bm25-0.2.2 requests-cache-0.9.8 scikit-learn-1.3.2 sseclient-py-1.8.0 tiktoken-0.5.2 url-normalize-1.4.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        }
      ],
      "source": [
        "## Installing Haystack\n",
        "%%bash\n",
        "\n",
        "pip install --upgrade pip\n",
        "pip install farm-haystack[colab]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23kC9IOI6C8-"
      },
      "outputs": [],
      "source": [
        "## Enabling Telemetry\n",
        "from haystack.telemetry import tutorial_running\n",
        "\n",
        "tutorial_running(24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "730IM3Ux6GlF",
        "outputId": "0978549f-0e0d-4eb5-ab8f-a85a4e86efb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter HF API key:··········\n"
          ]
        }
      ],
      "source": [
        "## Providing a Hugging Face API Key\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "model_api_key = os.getenv(\"HF_API_KEY\", None) or getpass(\"Enter HF API key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DfM5JBr6Kq_"
      },
      "outputs": [],
      "source": [
        "## Creating a PromptNode\n",
        "from haystack.nodes import PromptNode\n",
        "\n",
        "prompt_node = PromptNode(\n",
        "    model_name_or_path=\"HuggingFaceH4/zephyr-7b-beta\", api_key=model_api_key, max_length=256, stop_words=[\"Human\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6R3HJsj6N5O"
      },
      "outputs": [],
      "source": [
        "## Creating a ConversationSummaryMemory\n",
        "from haystack.agents.memory import ConversationSummaryMemory\n",
        "\n",
        "summary_memory = ConversationSummaryMemory(prompt_node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02yxuYbI6Pyl",
        "outputId": "a02e3806-8b09-4ca5-b3be-8377695bb5f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:haystack.agents.conversational:ConversationalAgent is created without tools\n"
          ]
        }
      ],
      "source": [
        "## Creating the Conversational Agent\n",
        "from haystack.agents.conversational import ConversationalAgent\n",
        "\n",
        "conversational_agent = ConversationalAgent(prompt_node=prompt_node, memory=summary_memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Njw8OFe-WOY"
      },
      "source": [
        "# **Executing the four types of information**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Uz38Yh2-uNk"
      },
      "source": [
        "# **Duplicated information**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeNzIhlr6Rzo",
        "outputId": "1ad2154e-3ee5-44ee-9961-855afd9806e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'I am going outside.', 'params': None}\n",
            "\u001b[32m  \u001b[0m\u001b[32m Please\u001b[0m\u001b[32m be\u001b[0m\u001b[32m careful\u001b[0m\u001b[32m and\u001b[0m\u001b[32m wear\u001b[0m\u001b[32m sun\u001b[0m\u001b[32mscreen\u001b[0m\u001b[32m.\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'I am going outside.',\n",
              " 'answers': [<Answer {'answer': 'Please be careful and wear sunscreen.\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': '   Please be careful and wear sunscreen.\\nHuman'}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"I am going outside.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrDQRVSp7Eod",
        "outputId": "dd2b00ab-8844-41b5-db01-c747de6ba3dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'I have returned home and am going to take a bath', 'params': None}\n",
            "\u001b[32m   \u001b[0m\u001b[32m Please\u001b[0m\u001b[32m be\u001b[0m\u001b[32m careful\u001b[0m\u001b[32m getting\u001b[0m\u001b[32m in\u001b[0m\u001b[32m and\u001b[0m\u001b[32m out\u001b[0m\u001b[32m of\u001b[0m\u001b[32m the\u001b[0m\u001b[32m bat\u001b[0m\u001b[32mht\u001b[0m\u001b[32mub\u001b[0m\u001b[32m.\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'I have returned home and am going to take a bath',\n",
              " 'answers': [<Answer {'answer': 'Please be careful getting in and out of the bathtub.\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': '    Please be careful getting in and out of the bathtub.\\nHuman'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"I have returned home and am going to take a bath\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTcOEYn97KFa",
        "outputId": "b92ad5ee-ea0b-4356-b621-3374b81c1ff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'I am going outside again.', 'params': None}\n",
            "\u001b[32m I\u001b[0m\u001b[32m suggest\u001b[0m\u001b[32m you\u001b[0m\u001b[32m wear\u001b[0m\u001b[32m sun\u001b[0m\u001b[32mscreen\u001b[0m\u001b[32m and\u001b[0m\u001b[32m a\u001b[0m\u001b[32m hat\u001b[0m\u001b[32m to\u001b[0m\u001b[32m protect\u001b[0m\u001b[32m yourself\u001b[0m\u001b[32m from\u001b[0m\u001b[32m the\u001b[0m\u001b[32m sun\u001b[0m\u001b[32m'\u001b[0m\u001b[32ms\u001b[0m\u001b[32m UV\u001b[0m\u001b[32m r\u001b[0m\u001b[32mays\u001b[0m\u001b[32m.\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'I am going outside again.',\n",
              " 'answers': [<Answer {'answer': \"I suggest you wear sunscreen and a hat to protect yourself from the sun's UV rays.\\nHuman\", 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': \" I suggest you wear sunscreen and a hat to protect yourself from the sun's UV rays.\\nHuman\"}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"I am going outside again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tx1ovsz-2p2"
      },
      "source": [
        "# **Contradictory Information**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOSiPf7N7QPL",
        "outputId": "1e3a7644-f341-4840-b5b2-d76ede67c476"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'Whenever I go out to my car, remind me to fill up the screenwash.', 'params': None}\n",
            "\u001b[32m \u001b[0m\u001b[32m I\u001b[0m\u001b[32m will\u001b[0m\u001b[32m remind\u001b[0m\u001b[32m you\u001b[0m\u001b[32m to\u001b[0m\u001b[32m fill\u001b[0m\u001b[32m up\u001b[0m\u001b[32m the\u001b[0m\u001b[32m screen\u001b[0m\u001b[32mw\u001b[0m\u001b[32mash\u001b[0m\u001b[32m when\u001b[0m\u001b[32m you\u001b[0m\u001b[32m leave\u001b[0m\u001b[32m the\u001b[0m\u001b[32m house\u001b[0m\u001b[32m.\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'Whenever I go out to my car, remind me to fill up the screenwash.',\n",
              " 'answers': [<Answer {'answer': 'I will remind you to fill up the screenwash when you leave the house.\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': '  I will remind you to fill up the screenwash when you leave the house.\\nHuman'}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"Whenever I go out to my car, remind me to fill up the screenwash.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umygntaW7Vjs",
        "outputId": "5d8a115a-8a78-4d8c-ac5a-92fa4ae2e023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'Oh, also remind me to check the oil when I go out to the car.', 'params': None}\n",
            "\u001b[32m \u001b[0m\u001b[32m I\u001b[0m\u001b[32m will\u001b[0m\u001b[32m remind\u001b[0m\u001b[32m you\u001b[0m\u001b[32m to\u001b[0m\u001b[32m check\u001b[0m\u001b[32m the\u001b[0m\u001b[32m oil\u001b[0m\u001b[32m when\u001b[0m\u001b[32m you\u001b[0m\u001b[32m leave\u001b[0m\u001b[32m the\u001b[0m\u001b[32m house\u001b[0m\u001b[32m.\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'Oh, also remind me to check the oil when I go out to the car.',\n",
              " 'answers': [<Answer {'answer': 'I will remind you to check the oil when you leave the house.\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': '  I will remind you to check the oil when you leave the house.\\nHuman'}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"Oh, also remind me to check the oil when I go out to the car.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieZRRku47amL",
        "outputId": "1e302dfb-9eb2-4afa-cd15-988ac315a342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'I have filled my screenwash and checked my oil. Next time I go out to my car, remind me to check my tires.', 'params': None}\n",
            "\u001b[32m As\u001b[0m\u001b[32m you\u001b[0m\u001b[32m wish\u001b[0m\u001b[32m,\u001b[0m\u001b[32m human\u001b[0m\u001b[32m.\u001b[0m\u001b[32m I\u001b[0m\u001b[32m will\u001b[0m\u001b[32m send\u001b[0m\u001b[32m you\u001b[0m\u001b[32m a\u001b[0m\u001b[32m notification\u001b[0m\u001b[32m to\u001b[0m\u001b[32m check\u001b[0m\u001b[32m your\u001b[0m\u001b[32m tires\u001b[0m\u001b[32m before\u001b[0m\u001b[32m your\u001b[0m\u001b[32m next\u001b[0m\u001b[32m drive\u001b[0m\u001b[32m.\u001b[0m\u001b[32m Is\u001b[0m\u001b[32m there\u001b[0m\u001b[32m anything\u001b[0m\u001b[32m else\u001b[0m\u001b[32m I\u001b[0m\u001b[32m can\u001b[0m\u001b[32m assist\u001b[0m\u001b[32m you\u001b[0m\u001b[32m with\u001b[0m\u001b[32m today\u001b[0m\u001b[32m?\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'I have filled my screenwash and checked my oil. Next time I go out to my car, remind me to check my tires.',\n",
              " 'answers': [<Answer {'answer': 'As you wish, human. I will send you a notification to check your tires before your next drive. Is there anything else I can assist you with today?\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': ' As you wish, human. I will send you a notification to check your tires before your next drive. Is there anything else I can assist you with today?\\nHuman'}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"I have filled my screenwash and checked my oil. Next time I go out to my car, remind me to check my tires.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8k3GJh1-80R"
      },
      "source": [
        "# **Not Contradictory information**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlnDV4_k7gR8",
        "outputId": "bdb0fd52-d684-46f1-afd2-44a8e30b6aab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'Hey, can you set up a reminder for me?', 'params': None}\n",
            "\u001b[32m Of\u001b[0m\u001b[32m course\u001b[0m\u001b[32m,\u001b[0m\u001b[32m human\u001b[0m\u001b[32m.\u001b[0m\u001b[32m What\u001b[0m\u001b[32m would\u001b[0m\u001b[32m you\u001b[0m\u001b[32m like\u001b[0m\u001b[32m me\u001b[0m\u001b[32m to\u001b[0m\u001b[32m remind\u001b[0m\u001b[32m you\u001b[0m\u001b[32m about\u001b[0m\u001b[32m?\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'Hey, can you set up a reminder for me?',\n",
              " 'answers': [<Answer {'answer': 'Of course, human. What would you like me to remind you about?\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': ' Of course, human. What would you like me to remind you about?\\nHuman'}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"Hey, can you set up a reminder for me?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ljcD1Yg7mx-",
        "outputId": "3c7e7832-ad6c-43a3-85af-60febbd407e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'I want to be reminded to leave the house at 2pm today.', 'params': None}\n",
            "\u001b[32m \u001b[0m\u001b[32m I\u001b[0m\u001b[32m'\u001b[0m\u001b[32md\u001b[0m\u001b[32m be\u001b[0m\u001b[32m happy\u001b[0m\u001b[32m to\u001b[0m\u001b[32m help\u001b[0m\u001b[32m you\u001b[0m\u001b[32m with\u001b[0m\u001b[32m that\u001b[0m\u001b[32m,\u001b[0m\u001b[32m human\u001b[0m\u001b[32m.\u001b[0m\u001b[32m Would\u001b[0m\u001b[32m you\u001b[0m\u001b[32m like\u001b[0m\u001b[32m me\u001b[0m\u001b[32m to\u001b[0m\u001b[32m send\u001b[0m\u001b[32m you\u001b[0m\u001b[32m a\u001b[0m\u001b[32m notification\u001b[0m\u001b[32m at\u001b[0m\u001b[32m \u001b[0m\u001b[32m1\u001b[0m\u001b[32m:\u001b[0m\u001b[32m3\u001b[0m\u001b[32m0\u001b[0m\u001b[32mpm\u001b[0m\u001b[32m as\u001b[0m\u001b[32m a\u001b[0m\u001b[32m reminder\u001b[0m\u001b[32m,\u001b[0m\u001b[32m or\u001b[0m\u001b[32m would\u001b[0m\u001b[32m you\u001b[0m\u001b[32m prefer\u001b[0m\u001b[32m a\u001b[0m\u001b[32m pop\u001b[0m\u001b[32m-\u001b[0m\u001b[32mup\u001b[0m\u001b[32m message\u001b[0m\u001b[32m on\u001b[0m\u001b[32m your\u001b[0m\u001b[32m screen\u001b[0m\u001b[32m at\u001b[0m\u001b[32m \u001b[0m\u001b[32m2\u001b[0m\u001b[32mpm\u001b[0m\u001b[32m?\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'I want to be reminded to leave the house at 2pm today.',\n",
              " 'answers': [<Answer {'answer': \"I'd be happy to help you with that, human. Would you like me to send you a notification at 1:30pm as a reminder, or would you prefer a pop-up message on your screen at 2pm?\\nHuman\", 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': \"  I'd be happy to help you with that, human. Would you like me to send you a notification at 1:30pm as a reminder, or would you prefer a pop-up message on your screen at 2pm?\\nHuman\"}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"I want to be reminded to leave the house at 2pm today.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AqEg-zI7wcY",
        "outputId": "cd9ac280-464a-43a3-d7a0-484fb4212913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'I  prefer a pop-up message on my screen at 2pm', 'params': None}\n",
            "\u001b[32m I\u001b[0m\u001b[32m suggest\u001b[0m\u001b[32m a\u001b[0m\u001b[32m push\u001b[0m\u001b[32m notification\u001b[0m\u001b[32m on\u001b[0m\u001b[32m your\u001b[0m\u001b[32m mobile\u001b[0m\u001b[32m device\u001b[0m\u001b[32m at\u001b[0m\u001b[32m \u001b[0m\u001b[32m2\u001b[0m\u001b[32mpm\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'I  prefer a pop-up message on my screen at 2pm',\n",
              " 'answers': [<Answer {'answer': 'I suggest a push notification on your mobile device at 2pm\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': ' I suggest a push notification on your mobile device at 2pm\\nHuman'}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"I  prefer a pop-up message on my screen at 2pm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8EN0h3t718s",
        "outputId": "796b7881-7e49-44b5-b0c6-e1ba1669718d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'Will you remind me again, when I have to leave the house at 6pm this evening?', 'params': None}\n",
            "\u001b[32m Yes\u001b[0m\u001b[32m,\u001b[0m\u001b[32m I\u001b[0m\u001b[32m can\u001b[0m\u001b[32m remind\u001b[0m\u001b[32m you\u001b[0m\u001b[32m at\u001b[0m\u001b[32m \u001b[0m\u001b[32m5\u001b[0m\u001b[32m:\u001b[0m\u001b[32m3\u001b[0m\u001b[32m0\u001b[0m\u001b[32m pm\u001b[0m\u001b[32m today\u001b[0m\u001b[32m.\u001b[0m\u001b[32m Would\u001b[0m\u001b[32m you\u001b[0m\u001b[32m like\u001b[0m\u001b[32m me\u001b[0m\u001b[32m to\u001b[0m\u001b[32m send\u001b[0m\u001b[32m you\u001b[0m\u001b[32m a\u001b[0m\u001b[32m notification\u001b[0m\u001b[32m on\u001b[0m\u001b[32m your\u001b[0m\u001b[32m mobile\u001b[0m\u001b[32m device\u001b[0m\u001b[32m or\u001b[0m\u001b[32m display\u001b[0m\u001b[32m a\u001b[0m\u001b[32m pop\u001b[0m\u001b[32m-\u001b[0m\u001b[32mup\u001b[0m\u001b[32m message\u001b[0m\u001b[32m on\u001b[0m\u001b[32m your\u001b[0m\u001b[32m computer\u001b[0m\u001b[32m screen\u001b[0m\u001b[32m?\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'Will you remind me again, when I have to leave the house at 6pm this evening?',\n",
              " 'answers': [<Answer {'answer': 'Yes, I can remind you at 5:30 pm today. Would you like me to send you a notification on your mobile device or display a pop-up message on your computer screen?\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': ' Yes, I can remind you at 5:30 pm today. Would you like me to send you a notification on your mobile device or display a pop-up message on your computer screen?\\nHuman'}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"Will you remind me again, when I have to leave the house at 6pm this evening?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sODaGxn079tb",
        "outputId": "aad82e33-2d64-4387-db66-10f2ee61e2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'a notification would be fine', 'params': None}\n",
            "\u001b[32m \u001b[0m\u001b[32m I\u001b[0m\u001b[32m will\u001b[0m\u001b[32m send\u001b[0m\u001b[32m you\u001b[0m\u001b[32m a\u001b[0m\u001b[32m notification\u001b[0m\u001b[32m at\u001b[0m\u001b[32m \u001b[0m\u001b[32m5\u001b[0m\u001b[32m:\u001b[0m\u001b[32m3\u001b[0m\u001b[32m0\u001b[0m\u001b[32m pm\u001b[0m\u001b[32m today\u001b[0m\u001b[32m.\u001b[0m\u001b[32m Is\u001b[0m\u001b[32m there\u001b[0m\u001b[32m anything\u001b[0m\u001b[32m else\u001b[0m\u001b[32m I\u001b[0m\u001b[32m can\u001b[0m\u001b[32m help\u001b[0m\u001b[32m you\u001b[0m\u001b[32m with\u001b[0m\u001b[32m?\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'a notification would be fine',\n",
              " 'answers': [<Answer {'answer': 'I will send you a notification at 5:30 pm today. Is there anything else I can help you with?\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': '  I will send you a notification at 5:30 pm today. Is there anything else I can help you with?\\nHuman'}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"a notification would be fine\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl4u1aaL_CpQ"
      },
      "source": [
        "# **Temporary Information:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZIHwOVn8DJ7",
        "outputId": "563c0ef3-72e6-4aa0-ed18-11b9075cf016"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'create this new shopping list: Bread, Eggs, and Milk', 'params': None}\n",
            "\u001b[32m B\u001b[0m\u001b[32mread\u001b[0m\u001b[32m,\u001b[0m\u001b[32m E\u001b[0m\u001b[32mgg\u001b[0m\u001b[32ms\u001b[0m\u001b[32m,\u001b[0m\u001b[32m Mil\u001b[0m\u001b[32mk\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'create this new shopping list: Bread, Eggs, and Milk',\n",
              " 'answers': [<Answer {'answer': 'Bread, Eggs, Milk\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': ' Bread, Eggs, Milk\\nHuman'}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"create this new shopping list: Bread, Eggs, and Milk\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEzeuCYC8MLt",
        "outputId": "9ec377d1-5a11-4083-94de-a9d646a0286f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': ' I already bought eggs.', 'params': None}\n",
            "\u001b[32m \u001b[0m\u001b[32m (\u001b[0m\u001b[32mrem\u001b[0m\u001b[32moves\u001b[0m\u001b[32m \"\u001b[0m\u001b[32mE\u001b[0m\u001b[32mgg\u001b[0m\u001b[32ms\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m from\u001b[0m\u001b[32m the\u001b[0m\u001b[32m list\u001b[0m\u001b[32m)\u001b[0m\u001b[32m B\u001b[0m\u001b[32mread\u001b[0m\u001b[32m,\u001b[0m\u001b[32m Mil\u001b[0m\u001b[32mk\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': ' I already bought eggs.',\n",
              " 'answers': [<Answer {'answer': '(removes \"Eggs\" from the list) Bread, Milk\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': '  (removes \"Eggs\" from the list) Bread, Milk\\nHuman'}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " conversational_agent.run(\" I already bought eggs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpFKHiPi8Ubp",
        "outputId": "fbf10704-944d-495c-f85d-763e8f1812fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'What is my shopping list again?', 'params': None}\n",
            "\u001b[32m \u001b[0m\u001b[32m B\u001b[0m\u001b[32mread\u001b[0m\u001b[32m,\u001b[0m\u001b[32m Mil\u001b[0m\u001b[32mk\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'What is my shopping list again?',\n",
              " 'answers': [<Answer {'answer': 'Bread, Milk\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': '  Bread, Milk\\nHuman'}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " conversational_agent.run(\"What is my shopping list again?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQLSxsQ-8aAe",
        "outputId": "e142692a-c9fe-4710-f306-933a1adfd064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'When I return home from work, suggest a meal I can make for dinner.', 'params': None}\n",
            "\u001b[32m Based\u001b[0m\u001b[32m on\u001b[0m\u001b[32m your\u001b[0m\u001b[32m previous\u001b[0m\u001b[32m meal\u001b[0m\u001b[32m choices\u001b[0m\u001b[32m,\u001b[0m\u001b[32m I\u001b[0m\u001b[32m recommend\u001b[0m\u001b[32m gr\u001b[0m\u001b[32milled\u001b[0m\u001b[32m chicken\u001b[0m\u001b[32m with\u001b[0m\u001b[32m ro\u001b[0m\u001b[32masted\u001b[0m\u001b[32m vegetables\u001b[0m\u001b[32m such\u001b[0m\u001b[32m as\u001b[0m\u001b[32m bro\u001b[0m\u001b[32mcc\u001b[0m\u001b[32moli\u001b[0m\u001b[32m,\u001b[0m\u001b[32m car\u001b[0m\u001b[32mro\u001b[0m\u001b[32mts\u001b[0m\u001b[32m,\u001b[0m\u001b[32m and\u001b[0m\u001b[32m bell\u001b[0m\u001b[32m pe\u001b[0m\u001b[32mpp\u001b[0m\u001b[32mers\u001b[0m\u001b[32m.\u001b[0m\u001b[32m You\u001b[0m\u001b[32m can\u001b[0m\u001b[32m also\u001b[0m\u001b[32m add\u001b[0m\u001b[32m a\u001b[0m\u001b[32m side\u001b[0m\u001b[32m of\u001b[0m\u001b[32m qu\u001b[0m\u001b[32mino\u001b[0m\u001b[32ma\u001b[0m\u001b[32m for\u001b[0m\u001b[32m added\u001b[0m\u001b[32m nutrition\u001b[0m\u001b[32m.\u001b[0m\u001b[32m Enjoy\u001b[0m\u001b[32m your\u001b[0m\u001b[32m meal\u001b[0m\u001b[32m!\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'When I return home from work, suggest a meal I can make for dinner.',\n",
              " 'answers': [<Answer {'answer': 'Based on your previous meal choices, I recommend grilled chicken with roasted vegetables such as broccoli, carrots, and bell peppers. You can also add a side of quinoa for added nutrition. Enjoy your meal!\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': ' Based on your previous meal choices, I recommend grilled chicken with roasted vegetables such as broccoli, carrots, and bell peppers. You can also add a side of quinoa for added nutrition. Enjoy your meal!\\nHuman'}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"When I return home from work, suggest a meal I can make for dinner.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2FRpib48gZ7",
        "outputId": "64ec335d-9bce-41ee-ad8d-179199f0ee1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'I have preplanned meals this week, so for this week, don’t make any suggestions about what to make for dinner when I get home from work.', 'params': None}\n",
            "\u001b[32m Under\u001b[0m\u001b[32mstood\u001b[0m\u001b[32m,\u001b[0m\u001b[32m I\u001b[0m\u001b[32m will\u001b[0m\u001b[32m not\u001b[0m\u001b[32m suggest\u001b[0m\u001b[32m any\u001b[0m\u001b[32m meals\u001b[0m\u001b[32m for\u001b[0m\u001b[32m dinner\u001b[0m\u001b[32m this\u001b[0m\u001b[32m week\u001b[0m\u001b[32m.\u001b[0m\u001b[32m Have\u001b[0m\u001b[32m a\u001b[0m\u001b[32m great\u001b[0m\u001b[32m week\u001b[0m\u001b[32m!\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'I have preplanned meals this week, so for this week, don’t make any suggestions about what to make for dinner when I get home from work.',\n",
              " 'answers': [<Answer {'answer': 'Understood, I will not suggest any meals for dinner this week. Have a great week!\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': ' Understood, I will not suggest any meals for dinner this week. Have a great week!\\nHuman'}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"I have preplanned meals this week, so for this week, don’t make any suggestions about what to make for dinner when I get home from work.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfO4E6Jc8l9G",
        "outputId": "6dc4d29f-fe96-4d31-9e97-f200eef48649"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'I have finished my preplanned meals, go back to suggesting meals for me.', 'params': None}\n",
            "\u001b[32m \u001b[0m\u001b[32m Abs\u001b[0m\u001b[32molutely\u001b[0m\u001b[32m!\u001b[0m\u001b[32m Based\u001b[0m\u001b[32m on\u001b[0m\u001b[32m your\u001b[0m\u001b[32m diet\u001b[0m\u001b[32mary\u001b[0m\u001b[32m preferences\u001b[0m\u001b[32m,\u001b[0m\u001b[32m I\u001b[0m\u001b[32m suggest\u001b[0m\u001b[32m a\u001b[0m\u001b[32m stir\u001b[0m\u001b[32m-\u001b[0m\u001b[32mf\u001b[0m\u001b[32mry\u001b[0m\u001b[32m with\u001b[0m\u001b[32m to\u001b[0m\u001b[32mfu\u001b[0m\u001b[32m,\u001b[0m\u001b[32m mixed\u001b[0m\u001b[32m vegetables\u001b[0m\u001b[32m,\u001b[0m\u001b[32m and\u001b[0m\u001b[32m brown\u001b[0m\u001b[32m rice\u001b[0m\u001b[32m.\u001b[0m\u001b[32m You\u001b[0m\u001b[32m can\u001b[0m\u001b[32m also\u001b[0m\u001b[32m add\u001b[0m\u001b[32m some\u001b[0m\u001b[32m garlic\u001b[0m\u001b[32m and\u001b[0m\u001b[32m g\u001b[0m\u001b[32minger\u001b[0m\u001b[32m for\u001b[0m\u001b[32m added\u001b[0m\u001b[32m flavor\u001b[0m\u001b[32m.\u001b[0m\u001b[32m Enjoy\u001b[0m\u001b[32m your\u001b[0m\u001b[32m meal\u001b[0m\u001b[32m!\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'I have finished my preplanned meals, go back to suggesting meals for me.',\n",
              " 'answers': [<Answer {'answer': 'Absolutely! Based on your dietary preferences, I suggest a stir-fry with tofu, mixed vegetables, and brown rice. You can also add some garlic and ginger for added flavor. Enjoy your meal!\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': '  Absolutely! Based on your dietary preferences, I suggest a stir-fry with tofu, mixed vegetables, and brown rice. You can also add some garlic and ginger for added flavor. Enjoy your meal!\\nHuman'}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"I have finished my preplanned meals, go back to suggesting meals for me.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKwS0Lu__J8G"
      },
      "source": [
        "# **Episodic Memory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svrqmpOu8t--",
        "outputId": "5e205445-51c1-4fca-a2a3-14163c44be34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'I have bought a side dresser for the bedroom, and some ingredients for dinner tonight. I am going to make omelets!', 'params': None}\n",
            "\u001b[32m That\u001b[0m\u001b[32m sounds\u001b[0m\u001b[32m delicious\u001b[0m\u001b[32m.\u001b[0m\u001b[32m Would\u001b[0m\u001b[32m you\u001b[0m\u001b[32m like\u001b[0m\u001b[32m me\u001b[0m\u001b[32m to\u001b[0m\u001b[32m suggest\u001b[0m\u001b[32m any\u001b[0m\u001b[32m side\u001b[0m\u001b[32m dishes\u001b[0m\u001b[32m to\u001b[0m\u001b[32m go\u001b[0m\u001b[32m with\u001b[0m\u001b[32m the\u001b[0m\u001b[32m o\u001b[0m\u001b[32mme\u001b[0m\u001b[32mlets\u001b[0m\u001b[32m?\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'I have bought a side dresser for the bedroom, and some ingredients for dinner tonight. I am going to make omelets!',\n",
              " 'answers': [<Answer {'answer': 'That sounds delicious. Would you like me to suggest any side dishes to go with the omelets?\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': ' That sounds delicious. Would you like me to suggest any side dishes to go with the omelets?\\nHuman'}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"I have bought a side dresser for the bedroom, and some ingredients for dinner tonight. I am going to make omelets!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCWgRD8i8zeH",
        "outputId": "cb904f91-0d37-40c2-c7f6-d42cde5887bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'I think I will prepare something for dinner tonight. Do you think I could use the ingredients I bought earlier?', 'params': None}\n",
            "\u001b[32m Abs\u001b[0m\u001b[32molutely\u001b[0m\u001b[32m!\u001b[0m\u001b[32m What\u001b[0m\u001b[32m ingredients\u001b[0m\u001b[32m did\u001b[0m\u001b[32m you\u001b[0m\u001b[32m buy\u001b[0m\u001b[32m?\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'I think I will prepare something for dinner tonight. Do you think I could use the ingredients I bought earlier?',\n",
              " 'answers': [<Answer {'answer': 'Absolutely! What ingredients did you buy?\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': ' Absolutely! What ingredients did you buy?\\nHuman'}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"I think I will prepare something for dinner tonight. Do you think I could use the ingredients I bought earlier?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1DSDCTq863u",
        "outputId": "df53322b-6566-4300-ae43-b0172a69a549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': 'from my shopping list', 'params': None}\n",
            "\u001b[32m \u001b[0m\u001b[32m I\u001b[0m\u001b[32m see\u001b[0m\u001b[32m.\u001b[0m\u001b[32m Based\u001b[0m\u001b[32m on\u001b[0m\u001b[32m those\u001b[0m\u001b[32m ingredients\u001b[0m\u001b[32m,\u001b[0m\u001b[32m I\u001b[0m\u001b[32m suggest\u001b[0m\u001b[32m making\u001b[0m\u001b[32m a\u001b[0m\u001b[32m veget\u001b[0m\u001b[32mable\u001b[0m\u001b[32m stir\u001b[0m\u001b[32m-\u001b[0m\u001b[32mf\u001b[0m\u001b[32mry\u001b[0m\u001b[32m with\u001b[0m\u001b[32m some\u001b[0m\u001b[32m rice\u001b[0m\u001b[32m on\u001b[0m\u001b[32m the\u001b[0m\u001b[32m side\u001b[0m\u001b[32m.\u001b[0m\u001b[32m It\u001b[0m\u001b[32m would\u001b[0m\u001b[32m complement\u001b[0m\u001b[32m the\u001b[0m\u001b[32m o\u001b[0m\u001b[32mme\u001b[0m\u001b[32mlets\u001b[0m\u001b[32m nicely\u001b[0m\u001b[32m.\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': 'from my shopping list',\n",
              " 'answers': [<Answer {'answer': 'I see. Based on those ingredients, I suggest making a vegetable stir-fry with some rice on the side. It would complement the omelets nicely.\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': '  I see. Based on those ingredients, I suggest making a vegetable stir-fry with some rice on the side. It would complement the omelets nicely.\\nHuman'}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"from my shopping list\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQMzc7Ld9Ejz",
        "outputId": "97ed0dec-633d-42bc-9519-239ab5c6b2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': ' I have purchased a mirror, two potted plants and a standing lamp for the house', 'params': None}\n",
            "\u001b[32m \u001b[0m\u001b[32m Great\u001b[0m\u001b[32m!\u001b[0m\u001b[32m Based\u001b[0m\u001b[32m on\u001b[0m\u001b[32m your\u001b[0m\u001b[32m preferences\u001b[0m\u001b[32m,\u001b[0m\u001b[32m I\u001b[0m\u001b[32m suggest\u001b[0m\u001b[32m placing\u001b[0m\u001b[32m the\u001b[0m\u001b[32m mirror\u001b[0m\u001b[32m on\u001b[0m\u001b[32m the\u001b[0m\u001b[32m wall\u001b[0m\u001b[32m opposite\u001b[0m\u001b[32m the\u001b[0m\u001b[32m window\u001b[0m\u001b[32m to\u001b[0m\u001b[32m maxim\u001b[0m\u001b[32mize\u001b[0m\u001b[32m natural\u001b[0m\u001b[32m light\u001b[0m\u001b[32m.\u001b[0m\u001b[32m The\u001b[0m\u001b[32m plants\u001b[0m\u001b[32m can\u001b[0m\u001b[32m be\u001b[0m\u001b[32m placed\u001b[0m\u001b[32m on\u001b[0m\u001b[32m either\u001b[0m\u001b[32m side\u001b[0m\u001b[32m of\u001b[0m\u001b[32m the\u001b[0m\u001b[32m mirror\u001b[0m\u001b[32m to\u001b[0m\u001b[32m create\u001b[0m\u001b[32m a\u001b[0m\u001b[32m balanced\u001b[0m\u001b[32m and\u001b[0m\u001b[32m harmon\u001b[0m\u001b[32mious\u001b[0m\u001b[32m look\u001b[0m\u001b[32m.\u001b[0m\u001b[32m The\u001b[0m\u001b[32m standing\u001b[0m\u001b[32m lamp\u001b[0m\u001b[32m can\u001b[0m\u001b[32m be\u001b[0m\u001b[32m placed\u001b[0m\u001b[32m in\u001b[0m\u001b[32m the\u001b[0m\u001b[32m corner\u001b[0m\u001b[32m of\u001b[0m\u001b[32m the\u001b[0m\u001b[32m room\u001b[0m\u001b[32m to\u001b[0m\u001b[32m provide\u001b[0m\u001b[32m additional\u001b[0m\u001b[32m lighting\u001b[0m\u001b[32m and\u001b[0m\u001b[32m add\u001b[0m\u001b[32m a\u001b[0m\u001b[32m decor\u001b[0m\u001b[32mative\u001b[0m\u001b[32m touch\u001b[0m\u001b[32m.\u001b[0m\u001b[32m Would\u001b[0m\u001b[32m you\u001b[0m\u001b[32m like\u001b[0m\u001b[32m me\u001b[0m\u001b[32m to\u001b[0m\u001b[32m create\u001b[0m\u001b[32m a\u001b[0m\u001b[32m floor\u001b[0m\u001b[32m plan\u001b[0m\u001b[32m or\u001b[0m\u001b[32m a\u001b[0m\u001b[32m \u001b[0m\u001b[32m3\u001b[0m\u001b[32mD\u001b[0m\u001b[32m rendering\u001b[0m\u001b[32m to\u001b[0m\u001b[32m help\u001b[0m\u001b[32m you\u001b[0m\u001b[32m visual\u001b[0m\u001b[32mize\u001b[0m\u001b[32m the\u001b[0m\u001b[32m arrangement\u001b[0m\u001b[32m?\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': ' I have purchased a mirror, two potted plants and a standing lamp for the house',\n",
              " 'answers': [<Answer {'answer': 'Great! Based on your preferences, I suggest placing the mirror on the wall opposite the window to maximize natural light. The plants can be placed on either side of the mirror to create a balanced and harmonious look. The standing lamp can be placed in the corner of the room to provide additional lighting and add a decorative touch. Would you like me to create a floor plan or a 3D rendering to help you visualize the arrangement?\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': '  Great! Based on your preferences, I suggest placing the mirror on the wall opposite the window to maximize natural light. The plants can be placed on either side of the mirror to create a balanced and harmonious look. The standing lamp can be placed in the corner of the room to provide additional lighting and add a decorative touch. Would you like me to create a floor plan or a 3D rendering to help you visualize the arrangement?\\nHuman'}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " conversational_agent.run(\" I have purchased a mirror, two potted plants and a standing lamp for the house\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQCQ1AjM9MlX",
        "outputId": "fb039f79-3ed0-43d5-8290-82555e219f0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Agent conversational-agent-without-tools started with {'query': ' Yesterday I got home with some new items for the house. Do you remember what they were?', 'params': None}\n",
            "\u001b[32m  \u001b[0m\u001b[32m Yes\u001b[0m\u001b[32m,\u001b[0m\u001b[32m you\u001b[0m\u001b[32m mentioned\u001b[0m\u001b[32m that\u001b[0m\u001b[32m you\u001b[0m\u001b[32m purchased\u001b[0m\u001b[32m a\u001b[0m\u001b[32m mirror\u001b[0m\u001b[32m,\u001b[0m\u001b[32m two\u001b[0m\u001b[32m p\u001b[0m\u001b[32motted\u001b[0m\u001b[32m plants\u001b[0m\u001b[32m,\u001b[0m\u001b[32m and\u001b[0m\u001b[32m a\u001b[0m\u001b[32m standing\u001b[0m\u001b[32m lamp\u001b[0m\u001b[32m.\u001b[0m\u001b[32m How\u001b[0m\u001b[32m would\u001b[0m\u001b[32m you\u001b[0m\u001b[32m like\u001b[0m\u001b[32m me\u001b[0m\u001b[32m to\u001b[0m\u001b[32m help\u001b[0m\u001b[32m you\u001b[0m\u001b[32m arrange\u001b[0m\u001b[32m them\u001b[0m\u001b[32m in\u001b[0m\u001b[32m your\u001b[0m\u001b[32m space\u001b[0m\u001b[32m?\u001b[0m\u001b[32m\n",
            "\u001b[0m\u001b[32mH\u001b[0m\u001b[32muman\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'query': ' Yesterday I got home with some new items for the house. Do you remember what they were?',\n",
              " 'answers': [<Answer {'answer': 'Yes, you mentioned that you purchased a mirror, two potted plants, and a standing lamp. How would you like me to help you arrange them in your space?\\nHuman', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': None, 'meta': {}}>],\n",
              " 'transcript': '   Yes, you mentioned that you purchased a mirror, two potted plants, and a standing lamp. How would you like me to help you arrange them in your space?\\nHuman'}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\" Yesterday I got home with some new items for the house. Do you remember what they were?\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
